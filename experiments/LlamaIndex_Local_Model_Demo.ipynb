{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d8e02c06",
      "metadata": {
        "id": "d8e02c06"
      },
      "source": [
        "# LlamaIndex - Private Setup\n",
        "\n",
        "Using GPT4ALL and our HuggingFace embeddings, we will injest [Chapter 3 of the recent IPCC Climate Report](https://www.ipcc.ch/report/ar6/wg2/chapter/chapter-3/), which covers oceans and coastal ecosystems. Using llama-index, this PDF is injested and vectorized, and questions can be answered about anything from this 172 paged report.\n",
        "\n",
        "Climate reports are long and tedious to read, so this demo will also help give some insight to the latest findings from the IPCC!\n",
        "\n",
        "Inspired by the recent popularity of [PrivateGPT](https://github.com/imartinez/privateGPT), this notebook will walk you through a llama-index setup that uses entirely local models. In this notebook, we use GPT4ALL and huggingface embeddings, which should run decently well on CPU alone. If you had more resources, we also provide some links further down for setting up any LLM from huggingface and running on GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aLc2fT7-NxhW",
      "metadata": {
        "id": "aLc2fT7-NxhW"
      },
      "source": [
        "## Dependencies Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jhRDdV5ItXQI",
      "metadata": {
        "id": "jhRDdV5ItXQI"
      },
      "source": [
        "### Setup Line Wrapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "l1gKZfTItWdf",
      "metadata": {
        "id": "l1gKZfTItWdf"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dbb071f",
      "metadata": {
        "id": "9dbb071f"
      },
      "source": [
        "### Download gpt4all model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f323fb5a",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "f323fb5a",
        "outputId": "4890177d-92ba-4296-c794-08b5470d07c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-05-15 17:52:45--  https://gpt4all.io/models/ggml-gpt4all-j-v1.3-groovy.bin\n",
            "Resolving gpt4all.io (gpt4all.io)... 172.67.71.169, 104.26.1.159, 104.26.0.159, ...\n",
            "Connecting to gpt4all.io (gpt4all.io)|172.67.71.169|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3785248281 (3.5G)\n",
            "Saving to: ‘ggml-gpt4all-j-v1.3-groovy.bin’\n",
            "\n",
            "ggml-gpt4all-j-v1.3 100%[===================>]   3.52G  60.2MB/s    in 71s     \n",
            "\n",
            "2023-05-15 17:53:57 (50.7 MB/s) - ‘ggml-gpt4all-j-v1.3-groovy.bin’ saved [3785248281/3785248281]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://gpt4all.io/models/ggml-gpt4all-j-v1.3-groovy.bin"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0248ed6",
      "metadata": {
        "id": "b0248ed6"
      },
      "source": [
        "### Download 2023 IPPC Climate Report - Chapter 3 on Oceans and Coastal Ecosystems (172 Pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "32830f2d",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "32830f2d",
        "outputId": "1e3ba715-002d-47e8-b4fc-35fd33a768bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-05-15 17:55:18--  https://www.ipcc.ch/report/ar6/wg2/downloads/report/IPCC_AR6_WGII_Chapter03.pdf\n",
            "Resolving www.ipcc.ch (www.ipcc.ch)... 104.20.23.161, 172.67.16.59, 104.20.24.161, ...\n",
            "Connecting to www.ipcc.ch (www.ipcc.ch)|104.20.23.161|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21752444 (21M) [application/pdf]\n",
            "Saving to: ‘IPCC_AR6_WGII_Chapter03.pdf’\n",
            "\n",
            "IPCC_AR6_WGII_Chapt 100%[===================>]  20.74M  5.51MB/s    in 3.8s    \n",
            "\n",
            "2023-05-15 17:55:22 (5.41 MB/s) - ‘IPCC_AR6_WGII_Chapter03.pdf’ saved [21752444/21752444]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.ipcc.ch/report/ar6/wg2/downloads/report/IPCC_AR6_WGII_Chapter03.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "864e6a4b",
      "metadata": {
        "id": "864e6a4b"
      },
      "source": [
        "### Download extra packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "06a356e7",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "06a356e7",
        "outputId": "3e8170f7-0663-46a7-9354-f1429c5d533a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymupdf in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (1.21.1)\n",
            "Collecting pygpt4all\n",
            "  Downloading pygpt4all-1.1.0.tar.gz (4.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting llama-index\n",
            "  Downloading llama_index-0.6.7-py3-none-any.whl (389 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.9/389.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting sentence_transformers\n",
            "  Using cached sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: accelerate in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (0.17.1)\n",
            "Collecting pyllamacpp\n",
            "  Downloading pyllamacpp-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (273 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.3/273.3 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pygptj\n",
            "  Downloading pygptj-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (246 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.0/246.0 kB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dataclasses-json in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from llama-index) (0.5.7)\n",
            "Requirement already satisfied: requests<2.30.0 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from llama-index) (2.28.2)\n",
            "Requirement already satisfied: tiktoken in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from llama-index) (0.3.1)\n",
            "Requirement already satisfied: numpy in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from llama-index) (1.24.2)\n",
            "Requirement already satisfied: openai>=0.26.4 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from llama-index) (0.27.2)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from llama-index) (8.2.2)\n",
            "Requirement already satisfied: langchain>=0.0.154 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from llama-index) (0.0.166)\n",
            "Requirement already satisfied: pandas in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from llama-index) (1.5.3)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from sentence_transformers) (4.30.0.dev0)\n",
            "Requirement already satisfied: tqdm in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from sentence_transformers) (4.64.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from sentence_transformers) (2.0.1)\n",
            "Requirement already satisfied: torchvision in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from sentence_transformers) (0.13.1)\n",
            "Requirement already satisfied: scikit-learn in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from sentence_transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from sentence_transformers) (0.1.97)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from sentence_transformers) (0.14.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from accelerate) (23.0)\n",
            "Requirement already satisfied: psutil in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from accelerate) (5.9.4)\n",
            "Requirement already satisfied: pyyaml in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: filelock in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.10.0)\n",
            "Requirement already satisfied: fsspec in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from langchain>=0.0.154->llama-index) (1.10.6)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from langchain>=0.0.154->llama-index) (4.0.2)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from langchain>=0.0.154->llama-index) (1.2.4)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from langchain>=0.0.154->llama-index) (3.8.4)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from langchain>=0.0.154->llama-index) (2.8.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from langchain>=0.0.154->llama-index) (1.4.47)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from dataclasses-json->llama-index) (3.19.0)\n",
            "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from dataclasses-json->llama-index) (1.5.1)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from dataclasses-json->llama-index) (0.6.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from requests<2.30.0->llama-index) (1.26.14)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from requests<2.30.0->llama-index) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from requests<2.30.0->llama-index) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from requests<2.30.0->llama-index) (2022.12.7)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (8.5.0.96)\n",
            "Requirement already satisfied: networkx in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (2.14.3)\n",
            "Requirement already satisfied: sympy in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (11.7.4.91)\n",
            "Requirement already satisfied: jinja2 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (11.7.101)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (11.7.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (11.4.0.1)\n",
            "Requirement already satisfied: setuptools in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence_transformers) (65.6.3)\n",
            "Requirement already satisfied: wheel in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence_transformers) (0.37.1)\n",
            "Requirement already satisfied: lit in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (15.0.7)\n",
            "Requirement already satisfied: cmake in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.26.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.10.31)\n",
            "Requirement already satisfied: joblib in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from nltk->sentence_transformers) (1.2.0)\n",
            "Requirement already satisfied: click in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from nltk->sentence_transformers) (8.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from pandas->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from pandas->llama-index) (2023.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from torchvision->sentence_transformers) (9.4.0)\n",
            "Collecting torch>=1.6.0\n",
            "  Using cached torch-1.12.1-cp310-cp310-manylinux1_x86_64.whl (776.3 MB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.154->llama-index) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.154->llama-index) (6.0.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.154->llama-index) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.154->llama-index) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.154->llama-index) (1.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->llama-index) (1.16.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain>=0.0.154->llama-index) (2.0.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json->llama-index) (1.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/Startupcolors/miniconda3/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "Building wheels for collected packages: pygpt4all, sentence_transformers\n",
            "  Building wheel for pygpt4all (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for pygpt4all: filename=pygpt4all-1.1.0-py3-none-any.whl size=5846 sha256=092dc734f00228b782af52653b3207c5927778da73e1451690d5837285148af1\n",
            "  Stored in directory: /home/Startupcolors/.cache/pip/wheels/7f/28/5c/336b6a4fb972a1ce86beee6222db2f01cfc33fd5b9046ba434\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125925 sha256=967a581995c45c43a1dd57e8820eb1dd91323ae09b0f85fa3593b49cd4512290\n",
            "  Stored in directory: /home/Startupcolors/.cache/pip/wheels/0a/f5/dd/9d00836c4e9e279c2a59d5b0ab72dafa66cbc626a327c550dd\n",
            "Successfully built pygpt4all sentence_transformers\n",
            "Installing collected packages: torch, pyllamacpp, pygptj, pygpt4all, sentence_transformers, llama-index\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1\n",
            "    Uninstalling torch-2.0.1:\n",
            "      Successfully uninstalled torch-2.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.0.1 requires torch==2.0.0, but you have torch 1.12.1 which is incompatible.\n",
            "peft 0.4.0.dev0 requires torch>=1.13.0, but you have torch 1.12.1 which is incompatible.\n",
            "icetk 0.0.5 requires protobuf<3.19, but you have protobuf 3.20.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed llama-index-0.6.7 pygpt4all-1.1.0 pygptj-2.0.3 pyllamacpp-2.1.3 sentence_transformers-2.2.2 torch-1.12.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pymupdf pygpt4all llama-index sentence_transformers accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b134f43",
      "metadata": {
        "id": "3b134f43"
      },
      "source": [
        "## Documents setup\n",
        "\n",
        "Here, we use PyMuPDFReader to quickly load all 172 pages of the climate report PDF. The `metadata=True` option will automatically set some helpful information like page numbers and filename, to help us keep track of sources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3323ec57",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3323ec57",
        "outputId": "13e44932-7764-44af-a6f9-5c8fe93ab683"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/Startupcolors/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.llms import GPT4All\n",
        "from llama_index.node_parser.simple import SimpleNodeParser\n",
        "from llama_index.langchain_helpers.text_splitter import TokenTextSplitter\n",
        "from llama_index import (\n",
        "    GPTVectorStoreIndex, \n",
        "    LangchainEmbedding, \n",
        "    LLMPredictor, \n",
        "    ServiceContext, \n",
        "    StorageContext, \n",
        "    download_loader,\n",
        "    PromptHelper\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "12bcb683",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "12bcb683",
        "outputId": "d41644ad-01e1-4809-fd13-20ccb1a596f8"
      },
      "outputs": [],
      "source": [
        "PyMuPDFReader = download_loader(\"PyMuPDFReader\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8fe03a70",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8fe03a70",
        "outputId": "e42dfba1-193a-454a-d262-26f2b9e65fd0"
      },
      "outputs": [],
      "source": [
        "documents = PyMuPDFReader().load(file_path='./IPCC_AR6_WGII_Chapter03.pdf', metadata=True)\n",
        "\n",
        "# ensure document texts are not bytes objects\n",
        "for doc in documents:\n",
        "    doc.text = doc.text.decode()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6963f753",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6963f753",
        "outputId": "75e01e1b-83c0-4613-ef38-482e56f27592"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(text='3\\n389\\nOceans and Coastal Ecosystems and Their Services  \\nChapter 3\\noverlapping climate-induced drivers and non-climate drivers confound \\nimplementation and assessment of the success of marine adaptation, \\nrevealing the complexity of attempting to maintain marine ecosystems \\nand services through adaptation. SROCC assessed with high confidence \\nthat while the benefits of many locally implemented adaptations exceed \\ntheir disadvantages, others are marginally effective and have large \\ndisadvantages, and overall, adaptation has a limited ability to reduce the \\nprobable risks from climate change, being at best a temporary solution \\n(Bindoff et\\xa0al., 2019a). SROCC also concluded that a portfolio of many \\ndifferent types of adaptation actions, effective and inclusive governance, \\nand mitigation must be combined for successful adaptation (Bindoff \\net\\xa0al., 2019a). The portfolio of adaptation measures has now been defined \\n(Section\\xa03.6.2), and individual and combined adaptation solutions have \\nbeen implemented in several marine sectors (Section\\xa03.6.3). Delays in \\nmarine adaptation have been partly attributed to the complexity of \\nocean governance (Section\\xa03.6.4; Cross-Chapter Box\\xa03 and Figure\\xa0CB3.1 \\nin Abram et\\xa0al., 2019) and to the low priority accorded the ocean in \\ninternational development goals (Nash et\\xa0al., 2020), but in recent\\xa0years \\nthe ocean is being increasingly incorporated in international climate \\npolicy and multilateral environmental agreements (Section\\xa03.6.4).\\nThis chapter assesses the current understanding of climate-induced \\ndrivers, ecological vulnerability and adaptability, risks to coastal \\nand ocean ecosystems, and human vulnerability and adaptation \\nto resulting changes in ocean benefits, now and in the future \\n(Figure\\xa03.2). It starts by assessing the biologically relevant outcomes \\nof anthropogenic climate-induced drivers (Section\\xa0 3.2). Next, it \\nsets out the mechanisms that determine the responses of ocean \\nand coastal organisms to individual and combined drivers from the \\ngenetic to the ecosystem level (Section\\xa03.3). This supports a detailed \\nassessment of the observed and projected responses of coastal and \\nocean ecosystems to these hazards, placing them in context using the \\npaleorecord (Section\\xa03.4). These observed and projected impacts are \\nused to quantify consequent risks to delivery of ecosystem services \\nand the socioeconomic sectors that depend on them, with attention \\nto the vulnerability, resilience and adaptive capacity of social–\\necological systems (Section\\xa03.5). The chapter concludes by assessing \\nthe state of adaptation and governance actions available to address \\nthese emerging threats while also advancing human development \\n(Section\\xa0 3.6). Abbreviations used repeatedly in the chapter are \\ndefined in Table\\xa03.1.\\nWGII AR6 Chapter 3 concept map\\n3.1\\n3.2\\n3.3\\n3.4\\n3.5\\n3.6\\nHeat, CO2\\n(see WGI)\\nHeatwaves\\nDeoxygenation\\nAcidification\\nOrganisms\\nPopulations\\nOcean and coastal systems\\nBiodiversity\\nEcosystem\\nservices\\n3.6\\nAdaptation\\nImplementation\\nBarriers\\nEnablers\\nPollution\\n(non-climate)\\nSDGs\\n3.4\\nProduction\\nSeasonality\\nBiodiversity\\n3.3\\nAcclimation\\nEvolution\\nExtinction\\nChanging properties\\nChanging mechanisms\\n3.5\\nAbundance\\nQuality\\nReliability\\nO2\\n3.2\\nStratification\\nCirculation\\nSea level rise\\nFigure\\xa03.2 | \\xa0WGII AR6 Chapter\\xa03 concept map. Climate changes both the properties (top of wave; Sections\\xa03.1–3.6) and the mechanisms (below wave; Sections\\xa03.2–3.6) \\nthat influence the ocean and coastal social–ecological system. The Sustainable Development Goals (top right) represent ideal outcomes and achievement of equitable, healthy and \\nsustainable ocean and coastal social–ecological systems.\\n', doc_id='f159f746-e19b-4cca-a8b1-123badf9fe07', embedding=None, doc_hash='d96ab78386530b777730c1e1911a19ef5d28ab1be54e761a13bcf30cf6f7820b', extra_info={'total_pages': 172, 'file_path': './IPCC_AR6_WGII_Chapter03.pdf', 'source': '11'})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# print a document to test. Each document is a single page from the pdf, with appropriate metadata\n",
        "documents[10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20cf0152",
      "metadata": {
        "id": "20cf0152"
      },
      "source": [
        "## CPU Llama Index\n",
        "The GPT4ALL setup follows the instructions from [langchain](https://python.langchain.com/en/latest/modules/models/llms/integrations/gpt4all.html).\n",
        "\n",
        "Then, the model is wrapped in the LLMPredictor class from llama-index. \n",
        "\n",
        "Keep in mind this current setup will run on CPU. If you have access to a GPU, you could also run any LLM from huggingface for improved speed and performance. More details available on huggingface LLMs and example notebooks [here](https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_llms.html#example-using-a-huggingface-llm).\n",
        "\n",
        "Lastly, the embeddings are downloaded and run locally using huggingface. These will automatically run on GPU if you have CUDA installed, otherwise they will run on CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "db16a4db",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "db16a4db",
        "outputId": "8b6d37be-ee93-41a0-c556-3605a225f91b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gptj_model_load: loading model from './ggml-gpt4all-j-v1.3-groovy.bin' - please wait ...\n",
            "gptj_model_load: n_vocab = 50400\n",
            "gptj_model_load: n_ctx   = 2048\n",
            "gptj_model_load: n_embd  = 4096\n",
            "gptj_model_load: n_head  = 16\n",
            "gptj_model_load: n_layer = 28\n",
            "gptj_model_load: n_rot   = 64\n",
            "gptj_model_load: f16     = 2\n",
            "gptj_model_load: ggml ctx size = 4505.45 MB\n",
            "gptj_model_load: memory_size =   896.00 MB, n_mem = 57344\n",
            "gptj_model_load: ................................... done\n",
            "gptj_model_load: model size =  3609.38 MB / num tensors = 285\n"
          ]
        }
      ],
      "source": [
        "local_llm_path = './ggml-gpt4all-j-v1.3-groovy.bin'\n",
        "llm = GPT4All(model=local_llm_path, backend='gptj', streaming=True, n_ctx=1024)\n",
        "llm_predictor = LLMPredictor(llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "322f882e",
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "e2b3e89df2824096900aa7b1ccbe6e07",
            "22fb642dfe29492d9518263faad8b5fc",
            "b740acb4c72e4f4190762644b3ff45a6",
            "532c4774e1f0471089ff88f1550743ff",
            "10c6610fc7b842fa9b37990a79416a8c",
            "cb4be68257d441759fad5fecb7040894",
            "52381c6f5bfb4501aafb2efcb2a07cac",
            "d753af86f9ff4f3ea21bc0af8343708a",
            "87d8c5f332c243fcb43ba8458d5636aa",
            "df374574a6344f96837fe2e0882bd95e",
            "354898da2e6243fc94533f45b6cac2b7",
            "f4a19885797040119a109abe0c23568b",
            "841072f44964444095a073585237f5dc",
            "789f6f4bdcab4c1cb8889f0c61602cf8"
          ]
        },
        "id": "322f882e",
        "outputId": "23c65324-d484-4110-9238-276960a5218b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading (…)a8e1d/.gitattributes: 100%|██████████| 1.18k/1.18k [00:00<00:00, 4.84MB/s]\n",
            "Downloading (…)_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 1.01MB/s]\n",
            "Downloading (…)b20bca8e1d/README.md: 100%|██████████| 10.6k/10.6k [00:00<00:00, 43.1MB/s]\n",
            "Downloading (…)0bca8e1d/config.json: 100%|██████████| 571/571 [00:00<00:00, 3.60MB/s]\n",
            "Downloading (…)ce_transformers.json: 100%|██████████| 116/116 [00:00<00:00, 755kB/s]\n",
            "Downloading (…)e1d/data_config.json: 100%|██████████| 39.3k/39.3k [00:00<00:00, 393kB/s]\n",
            "Downloading pytorch_model.bin: 100%|██████████| 438M/438M [00:04<00:00, 93.1MB/s] \n",
            "Downloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 246kB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100%|██████████| 239/239 [00:00<00:00, 1.54MB/s]\n",
            "Downloading (…)a8e1d/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 1.46MB/s]\n",
            "Downloading (…)okenizer_config.json: 100%|██████████| 363/363 [00:00<00:00, 1.97MB/s]\n",
            "Downloading (…)8e1d/train_script.py: 100%|██████████| 13.1k/13.1k [00:00<00:00, 30.6MB/s]\n",
            "Downloading (…)b20bca8e1d/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.12MB/s]\n",
            "Downloading (…)bca8e1d/modules.json: 100%|██████████| 349/349 [00:00<00:00, 1.57MB/s]\n"
          ]
        }
      ],
      "source": [
        "embed_model = LangchainEmbedding(HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "F2B-NBQXsT1K",
      "metadata": {
        "id": "F2B-NBQXsT1K"
      },
      "source": [
        "Note that due to model limitations (and considerations for speed), these settings are non-standard, but should help ensure reasonable resource usage with GPT4ALL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "fb773540",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fb773540",
        "outputId": "357fc08f-fbd3-4686-95e6-202a732d6017"
      },
      "outputs": [],
      "source": [
        "prompt_helper = PromptHelper(max_input_size=1024, num_output=256, max_chunk_overlap=-1000)\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    llm_predictor=llm_predictor,\n",
        "    embed_model=embed_model,\n",
        "    prompt_helper=prompt_helper,\n",
        "    node_parser=SimpleNodeParser(text_splitter=TokenTextSplitter(chunk_size=512, chunk_overlap=50))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "hLnxVbxTmAWU",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hLnxVbxTmAWU",
        "outputId": "7c15d7d8-c0fd-4ab4-ae65-9e21bdb8c766"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Hello! How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "# Quick sanity test that the LLM works (seems to only work if you run this here!)\n",
        "print(llm._call(\"hello world\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bcbfd55",
      "metadata": {
        "id": "6bcbfd55"
      },
      "source": [
        "### Create the Index\n",
        "\n",
        "This step will break each document into nodes, and create an embedding vector for each node using our `embed_model`. This may take a several minutes if running on CPU (this is a large climate report)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "fbe0af22",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fbe0af22",
        "outputId": "f178c788-5004-4959-90c1-f387e10be774"
      },
      "outputs": [],
      "source": [
        "index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ca6ae86f",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ca6ae86f",
        "outputId": "81ff3c34-78a7-4bf3-983c-76a24ac3945b"
      },
      "outputs": [],
      "source": [
        "index.storage_context.persist(persist_dir=\"./storage\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19b22499",
      "metadata": {
        "id": "19b22499"
      },
      "source": [
        "#### (Optional) Load the Index if already saved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "56f15812",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "56f15812",
        "outputId": "655fb3a0-8dcf-4bb5-c795-8669e0880476"
      },
      "outputs": [],
      "source": [
        "from llama_index import load_index_from_storage\n",
        "storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
        "index = load_index_from_storage(storage_context, service_context=service_context)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e5bc7d4",
      "metadata": {
        "id": "7e5bc7d4"
      },
      "source": [
        "### Try Asking a question\n",
        "\n",
        "Due to processing constraints, setting `similarity_top_k=1` is an ideal setting. Otherwise, responses will be quite slow due to the speed of CPU inference. Either way, expect to wait a minute or two before the response is streaming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "90aa65d3",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "90aa65d3",
        "outputId": "7e504006-8d6d-43ec-87f4-8879639b67ef"
      },
      "outputs": [],
      "source": [
        "query_engine = index.as_query_engine(streaming=True, similarity_top_k=1, service_context=service_context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5be63ef4",
      "metadata": {
        "id": "5be63ef4"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'query_engine' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response_stream \u001b[39m=\u001b[39m query_engine\u001b[39m.\u001b[39mquery(\u001b[39m\"\u001b[39m\u001b[39mWhat are the main climate risks to our Oceans?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m response_stream\u001b[39m.\u001b[39mprint_response_stream()\n",
            "\u001b[0;31mNameError\u001b[0m: name 'query_engine' is not defined"
          ]
        }
      ],
      "source": [
        "response_stream = query_engine.query(\"What are the main climate risks to our Oceans?\")\n",
        "response_stream.print_response_stream()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8VtevZVyweJ",
      "metadata": {
        "id": "d8VtevZVyweJ"
      },
      "source": [
        "The above response is hallucinated slightly, since all that was not availabe in the provided top-1 releveant node. In any case, it grounded the response and gave a generally OK response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kxAGK2v2PTHz",
      "metadata": {
        "id": "kxAGK2v2PTHz"
      },
      "source": [
        "## GPU Llama Index\n",
        "\n",
        "As stated earlier, if you have a modest GPU available (at least 15GB of VRAM), you can speed things up considerably.\n",
        "\n",
        "This next section will setup a new predictor from Huggingface, using the Writer/camel-5b-hf model (which is also conviently licensed for commericial use).\n",
        "\n",
        "(If you are running in colab, switch to a GPU instance first!)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MUrHv6CHt7cC",
      "metadata": {
        "id": "MUrHv6CHt7cC"
      },
      "source": [
        "### LLM + Embed Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J8lHs-mARtoq",
      "metadata": {
        "id": "J8lHs-mARtoq"
      },
      "outputs": [],
      "source": [
        "# setup prompts - specific to Camel\n",
        "from llama_index.prompts.prompts import SimpleInputPrompt\n",
        "\n",
        "# This will wrap the default prompts that are internal to llama-index\n",
        "# taken from https://huggingface.co/Writer/camel-5b-hf\n",
        "query_wrapper_prompt = SimpleInputPrompt(\n",
        "    \"Below is an instruction that describes a task. \"\n",
        "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "    \"### Instruction:\\n{query_str}\\n\\n### Response:\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81a3dac4",
      "metadata": {
        "id": "81a3dac4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from llama_index.llm_predictor import HuggingFaceLLMPredictor\n",
        "\n",
        "# NOTE: the first run of this will download/cache the weights, ~20GB\n",
        "hf_predictor = HuggingFaceLLMPredictor(\n",
        "    max_input_size=2048, \n",
        "    max_new_tokens=256,\n",
        "    temperature=0.25,\n",
        "    do_sample=False,\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    tokenizer_name=\"Writer/camel-5b-hf\",\n",
        "    model_name=\"Writer/camel-5b-hf\",\n",
        "    device_map=\"auto\",\n",
        "    tokenizer_kwargs={\"max_length\": 2048},\n",
        "    model_kwargs={\"torch_dtype\": torch.bfloat16}\n",
        ")\n",
        "\n",
        "embed_model = LangchainEmbedding(HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))\n",
        "\n",
        "service_context = ServiceContext.from_defaults(chunk_size_limit=512, llm_predictor=hf_predictor, embed_model=embed_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IG0bNaiITP52",
      "metadata": {
        "id": "IG0bNaiITP52"
      },
      "source": [
        "### Construct index using GPU\n",
        "\n",
        "Since we are using a GPU now, the emebddings are much faster to generate! Plus, now we are working with larger chunks, which enables the embeddings to represent the text and enable better retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XptBOyfSTKvN",
      "metadata": {
        "id": "XptBOyfSTKvN"
      },
      "outputs": [],
      "source": [
        "index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "index.storage_context.persist(persist_dir=\"./storage\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lIXNrx0NTaR8",
      "metadata": {
        "id": "lIXNrx0NTaR8"
      },
      "source": [
        "#### (Optional) Load if already saved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gTME1u2hTXbe",
      "metadata": {
        "id": "gTME1u2hTXbe"
      },
      "outputs": [],
      "source": [
        "from llama_index import load_index_from_storage\n",
        "storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
        "index = load_index_from_storage(storage_context, service_context=service_context)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MUX6E4DwTR4i",
      "metadata": {
        "id": "MUX6E4DwTR4i"
      },
      "source": [
        "### Query using GPU\n",
        "\n",
        "With a GPU, the response will begin streaming very quickly. Camel is an excellent model given it's modest size of 5B parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "v18rRQ-YSKgF",
      "metadata": {
        "id": "v18rRQ-YSKgF"
      },
      "outputs": [],
      "source": [
        "query_engine = index.as_query_engine(streaming=True, similarity_top_k=3, service_context=service_context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-7Ob0nZNSSRC",
      "metadata": {
        "id": "-7Ob0nZNSSRC"
      },
      "outputs": [],
      "source": [
        "response_stream = query_engine.query(\"What are the main climate risks to our Oceans?\")\n",
        "response_stream.print_response_stream()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "X1JOxYzHy_fq",
      "metadata": {
        "id": "X1JOxYzHy_fq"
      },
      "source": [
        "Compared to GPT4ALL, this response is more complete and accurate. You can inspect the source nodes below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zoW-q0_GzGCP",
      "metadata": {
        "id": "zoW-q0_GzGCP"
      },
      "outputs": [],
      "source": [
        "print(response_stream.source_nodes)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "aLc2fT7-NxhW",
        "9dbb071f",
        "b0248ed6",
        "864e6a4b",
        "3b134f43",
        "6bcbfd55"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
